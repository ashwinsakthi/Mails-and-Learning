Senior Data Engineer – R&D (Global AI Accelerator India)

 
https://github.com/jason4wy/ebook/blob/master/distributed%20system/Martin%20Kleppmann-Designing%20Data-Intensive%20Applications_%20The%20Big%20Ideas%20Behind%20Reliable%2C%20Scalable%2C%20and%20Maintainable%20Systems-O%E2%80%99Reilly%20Media%20(2017).pdf
 

Ericsson Overview:

Ericsson is world’s leading provider of communications technology and services. Our offerings include services, consulting, software and infrastructure within Information and Communications Technology.

Using innovation to empower people, business and society, Ericsson is working towards the Networked Society: a world connected in real time that will open up opportunities to create freedom, transform society and drive solutions to some of our planet’s greatest challenges.

We are truly a global company, operating across borders in over 180 countries, offering a diverse, performance-driven culture and an innovative and engaging environment. As an Ericsson employee, you will have freedom to think big and the support to turn ideas into achievements. Continuous learning and growth opportunities allow you to acquire the knowledge and skills necessary to progress and reach your career goals. We invite you to join our team.

 

Role Summary:

As a Senior Data Engineer,you shall be a part of our growing team of MI experts. As a team leader, you will be evolving and optimizing our data and data pipeline architecture, as well as, optimizing data flow and collection for cross functional teams. You are an expert data pipeline builder and data wrangler who enjoys optimizing data systems and evolving them. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data and models devOps (dataOps) architecture is consistent throughout ongoing projects. You are self-directed and comfortable supporting the dataOps needs of multiple teams, systems and products. You will also be responsible for integrating them with the architecture used across the company. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s dataOps architecture to support our existing and next generation of MI-driven products and solutions initiatives.

Key Responsibilities:

·         Create and maintain optimal data and model dataOps pipeline architecture

·         Assemble large, complex data sets that meet functional / non-functional business requirements.

·         Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

·         Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud-based ‘big data’ technologies from AWS, Azure and others.

·         Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

·         Keep data separated and secure across national boundaries through multiple data centers and strategic customers/partners.

·         Create tool-chains for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.

·         Work with data and machine learning experts to strive for greater functionality in our data and model life cycle management systems.

·         Support dataOps competence build-up in Ericsson Businesses and Customer Serving Units

Key Qualifications:

·         Bachelors/Masters/Ph.D. in Computer Science, Information Systems, Data Science, Artificial Intelligence, Machine Learning, Electrical Engineering or related disciplines from any of the reputed institutes. First Class, preferably with Distinction.

·         Overall industry experience of around 15 years, at least 5 years’ experience as a Data Engineer.

·         5+ years of experience in the following:

·         Software/tools:Hadoop, Spark, Kafka, etc.

·         Relational SQL and NoSQL databases, including Postgres and Cassandra.

·         Data and Model pipeline and workflow management tools: Azkaban, Luigi, Airflow, Dataiku, etc.

·         Stream-processing systems: Storm, Spark-Streaming, etc.

·         Object-oriented/object function scripting languages: Python, Java, Scala (Advanced level in one language, at least)

·         Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.

·         Experience performing root cause analysis on internal and external data and processes to answer specific business questions and seek opportunities for improvement.

·         Experience in Data warehouse design and dimensional modeling

·         Strong analytic skills related to working with unstructured datasets.

·         Experience building processes supporting data transformation, data structures, metadata, dependency and workload management.

·         Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of other databases/date-sources.

·         Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

·         Experience with Docker containers, orchestration systems (e.g. Kubernetes), continuous integration and job schedulers.

·         Familiar with functional programming and scripting languages such as Javascript or GO

·         Knowledge of server-less architectures (e.g. Lambda, Kinesis, Glue).

·         Experience with microservices and REST APIs.

·         Familiar with agile development and lean principles.

·         Contributor or owner of GitHub repo.

·         Strong project management and interpersonal skills.

·         Experience supporting and working with cross-functional teams in a dynamic environment.

·         Good communication skills in written and spoken English

·         Creativity and ability to formulate problems and solve them independently 

·         Ability to build and nurture internal and external communities

·         Experience in writing and presenting white papers, journal articles and technical blogs on the results

Additional Requirements:

·         Applications/Domain-knowledge in Telecommunication and/or IoT, a plus.

·         Experience with data visualization and dashboard creation is a plus

·         Ability to work independently with high energy, enthusiasm and persistence

·         Experience in partnering and collaborative co-creation, i.e., working with complex multiple stakeholder business units, global customers, technology and other ecosystem partners in a multi-culture, global matrix organization with sensitivity and persistence

 


There were three rounds of interviews, two telephonic and one f2f. Asked mainly abt the tools used in Data pipelines like Spark, Kafka, NoSQL, etc. QUestions also were based on very basics of Machine learning algorithms, not mandatory but good to know

